{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[FunctionDef(name='add', args=arguments(posonlyargs=[], args=[arg(arg='a', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))])), arg(arg='b', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))]))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='out', ctx=Store())], value=BinOp(left=Name(id='a', ctx=Load()), op=Add(), right=Name(id='b', ctx=Load()))), Return(value=Name(id='out', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='gpu'))])])], type_ignores=[])\n",
      "Visit Module\n",
      "Visit FunctionDef\n",
      "Visit arguments\n",
      "Visit Assign\n",
      "Visit Name\n",
      "Visit BinOp\n",
      "Visit Name\n",
      "Visit Name\n",
      "Visit Return\n",
      "Visit Name\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import relax as R\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @R.function\n",
      "    def add(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
      "        out: R.Tensor((2, 3), dtype=\"float32\") = R.add(a, b)\n",
      "        return out\n",
      "After applied passes...\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "# from tvm.script import relax as R\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func(private=True)\n",
      "    def add1(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_add: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
      "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "        # with T.block(\"root\"):\n",
      "        for ax0, ax1 in T.grid(T.int64(2), T.int64(3)):\n",
      "            with T.block(\"T_add\"):\n",
      "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
      "                T.reads(a[v_ax0, v_ax1], b[v_ax0, v_ax1])\n",
      "                T.writes(T_add[v_ax0, v_ax1])\n",
      "                T_add[v_ax0, v_ax1] = a[v_ax0, v_ax1] + b[v_ax0, v_ax1]\n",
      "\n",
      "    @R.function\n",
      "    def add(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
      "        cls = Module\n",
      "        out = R.call_tir(cls.add1, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
      "        return out\n",
      "After applied dlight...\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "# from tvm.script import relax as R\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func(private=True)\n",
      "    def add1(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_add: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
      "        T.func_attr({\"tir.is_scheduled\": 1, \"tir.noalias\": T.bool(True)})\n",
      "        # with T.block(\"root\"):\n",
      "        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread=\"blockIdx.x\"):\n",
      "            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n",
      "                with T.block(\"T_add\"):\n",
      "                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(3))\n",
      "                    v1 = T.axis.spatial(T.int64(3), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(3))\n",
      "                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(6))\n",
      "                    T.reads(a[v0, v1], b[v0, v1])\n",
      "                    T.writes(T_add[v0, v1])\n",
      "                    T_add[v0, v1] = a[v0, v1] + b[v0, v1]\n",
      "\n",
      "    @R.function\n",
      "    def add(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
      "        cls = Module\n",
      "        out = R.call_tir(cls.add1, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
      "        return out\n",
      "\n",
      "#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n",
      "     (__CUDACC_VER_MAJOR__ > 11))\n",
      "#define TVM_ENABLE_L2_PREFETCH 1\n",
      "#else\n",
      "#define TVM_ENABLE_L2_PREFETCH 0\n",
      "#endif\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) add1_kernel(float* __restrict__ T_add, float* __restrict__ a, float* __restrict__ b);\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) add1_kernel(float* __restrict__ T_add, float* __restrict__ a, float* __restrict__ b) {\n",
      "  if (((int)threadIdx.x) < 6) {\n",
      "    T_add[((int)threadIdx.x)] = (a[((int)threadIdx.x)] + b[((int)threadIdx.x)]);\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import ast\n",
    "import astunparse\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import tvm\n",
    "from tvm import dlight as dl\n",
    "from tvm import relax as rx\n",
    "from tvm.script import relax as R\n",
    "from tvm.script.ir_builder import relax as relax_builder, ir as I, IRBuilder as IB\n",
    "def jit(target=\"cpu\"):\n",
    "    assert target in [\"cpu\", \"gpu\"]\n",
    "    def inner(fn):\n",
    "        return JIT(fn, target=target)\n",
    "    return inner\n",
    "\n",
    "class JIT:\n",
    "    def __init__(self, fn, target=\"cpu\"):\n",
    "        self.fn = fn\n",
    "        self.target = target\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        fn_src = inspect.getsource(self.fn)\n",
    "        fn_ast = ast.parse(fn_src)\n",
    "        print(ast.dump(fn_ast))\n",
    "        ctx = self.fn.__globals__.copy()\n",
    "        code_generator = CodeGenerator(fn_ast, ctx, self.target)\n",
    "        compiled_kernel = code_generator.code_gen()\n",
    "        input_args = []\n",
    "        for arg in args:\n",
    "            input_args.append(arg.data)\n",
    "        return compiled_kernel(*input_args)\n",
    "\n",
    "class CodeGenerator(ast.NodeVisitor):\n",
    "    def __init__(self, fn_ast, ctx, target):\n",
    "        self.fn_ast = fn_ast\n",
    "        self.ctx = ctx\n",
    "        self.target = target\n",
    "        self.ib = IB()\n",
    "        self.ir_module = None\n",
    "        self.entry = None\n",
    "        self.ret = None\n",
    "        self.local_var_table : Dict[str, Any] = {}\n",
    "    \n",
    "    def code_gen(self):\n",
    "        with self.ib:\n",
    "            self.visit(self.fn_ast)\n",
    "        module = self.ib.get()\n",
    "        print(module)\n",
    "\n",
    "        # apply transform pass on module\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            # Apply Opt Pass\n",
    "            seq = tvm.transform.Sequential(\n",
    "                [\n",
    "                    rx.transform.LegalizeOps(),\n",
    "                ])\n",
    "            module = seq(module)\n",
    "        print(\"After applied passes...\")    \n",
    "        print(module)\n",
    "\n",
    "        mapped_target = {'cpu': 'llvm', 'gpu': 'cuda'}\n",
    "        target = tvm.target.Target(mapped_target[self.target])\n",
    "        if \"cuda\" in target.keys:\n",
    "            with target:\n",
    "                module = dl.ApplyDefaultSchedule(dl.gpu.Fallback(),)(module)  ## USE GPU\n",
    "            print(\"After applied dlight...\")\n",
    "            print(module)\n",
    "        \n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            ex = rx.build(module, target=target)\n",
    "        device = tvm.cuda() if \"cuda\" in target.keys else tvm.cpu()\n",
    "        vm = rx.VirtualMachine(ex, device=device)\n",
    "        print(ex.mod.imported_modules[0].imported_modules[0].get_source()) # print the source file\n",
    "        return vm[self.entry]\n",
    "\n",
    "\n",
    "    def visit(self, node):\n",
    "        print(\"Visit \" + node.__class__.__name__)\n",
    "        return super().visit(node)\n",
    "    \n",
    "    def visit_Module(self, node: ast.Module):\n",
    "        if self.ir_module:\n",
    "            raise AssertionError(\"We should have only one module!\")\n",
    "        self.ir_module = I.ir_module()\n",
    "        with self.ir_module:\n",
    "            super().generic_visit(node)\n",
    "        \n",
    "    \n",
    "    def visit_FunctionDef(self, node: ast.FunctionDef):\n",
    "        fn = relax_builder.function()\n",
    "        self.entry = node.name\n",
    "        with fn:\n",
    "            R.func_name(node.name)\n",
    "            self.visit(node.args)\n",
    "            self._visit_compound_stmt(node.body)\n",
    "\n",
    "            if self.ret is None:\n",
    "                R.func_ret_value(rx.ShapeExpr([]))\n",
    "            else:\n",
    "                R.func_ret_value(self.ret)\n",
    "    \n",
    "    def visit_arguments(self, node: ast.arguments):\n",
    "        for arg in node.args:\n",
    "            if arg.annotation is None:\n",
    "                raise ValueError(arg, \"Type annotation is required for function parameters.\")\n",
    "            arg_name = arg.arg\n",
    "            anno = eval(astunparse.unparse(arg.annotation), self.ctx)\n",
    "            param = R.arg(arg_name, R.Tensor(shape=anno.shape, dtype=anno.dtype))\n",
    "            self.local_var_table[arg_name] = param\n",
    "\n",
    "    def visit_Pass(self, node: ast.Pass):\n",
    "        pass\n",
    "\n",
    "    def visit_Assign(self, node: ast.Assign):\n",
    "        if len(node.targets) != 1:\n",
    "            raise NotImplementedError(\"Doesn't support simultaneous multiple assignment like 'a = b = c' in AST node type: {}\".format(type(node).__name__))\n",
    "        target: rx.Var = self.visit(node.targets[0])\n",
    "        value = self.visit(node.value)\n",
    "        self.local_var_table[target.name_hint] = value\n",
    "        self.ib.name(target.name_hint, value)\n",
    "    \n",
    "    def visit_Name(self, node: ast.Name):\n",
    "        name = node.id\n",
    "        if isinstance(node.ctx, ast.Store):\n",
    "            if name not in self.local_var_table.keys():\n",
    "                self.local_var_table[name] = rx.Var(name, struct_info=rx.ObjectStructInfo())\n",
    "        return self.local_var_table[name]\n",
    "    \n",
    "    def visit_BinOp(self, node: ast.BinOp):\n",
    "        lhs = self.visit(node.left)\n",
    "        rhs = self.visit(node.right)\n",
    "        return R.emit(self._binOp_maker(node.op)(lhs, rhs))\n",
    "    \n",
    "    def visit_Return(self, node: ast.Return):\n",
    "        ret_value = self.visit(node.value)\n",
    "        return ret_value\n",
    "    \n",
    "    def visit_Constant(self, node: ast.Constant):\n",
    "        return R.emit(rx.const(node.value))\n",
    "        \n",
    "    def _visit_compound_stmt(self, stmts):\n",
    "        assert isinstance(stmts, (list, tuple))\n",
    "        for stmt in stmts:\n",
    "            ret = self.visit(stmt)\n",
    "            if ret is not None and isinstance(stmt, ast.Return):\n",
    "                self.ret = ret\n",
    "    \n",
    "    def _binOp_maker(self, node: ast.operator):\n",
    "        if isinstance(node, ast.Add):\n",
    "            return R.add\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "    \n",
    "    def generic_visit(self, node: ast.AST):\n",
    "        raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, shape, dtype):\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self._data = None\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    @data.setter\n",
    "    def data(self, data: \"torch.Tensor\"):\n",
    "        def _from_dlpack(tensor):\n",
    "            from tvm.runtime import Device\n",
    "            from tvm.runtime import ndarray\n",
    "            try:\n",
    "                return ndarray.from_dlpack(tensor)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "            device_type = tensor.device.type\n",
    "            device_id = tensor.device.index or 0\n",
    "            return ndarray.array(\n",
    "                tensor.numpy(),\n",
    "                device=Device(\n",
    "                    Device.STR2MASK[device_type],\n",
    "                    device_id,\n",
    "                ),\n",
    "            )\n",
    "        data = _from_dlpack(data)\n",
    "        if data.shape != tuple(self.shape):\n",
    "            raise ValueError(f\"Shape mismatch: expected {tuple(self.shape)}, got {data.shape}\")\n",
    "        if data.dtype != self.dtype:\n",
    "            raise ValueError(f\"Dtype mismatch: expected {self.dtype}, got {data.dtype}\")\n",
    "        self._data = data\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dtype) + '[' + ', '.join(str(s) for s in self.shape) + ']'\n",
    "\n",
    "@jit(target=\"gpu\")\n",
    "def add(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")):\n",
    "    out = a + b\n",
    "    return out\n",
    "\n",
    "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "a.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
    "b.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
    "print(add(a, b))\n",
    "\n",
    "# NOTE: why thread ID 一定是 1024 ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyscf_isdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
