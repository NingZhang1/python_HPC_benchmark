{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[FunctionDef(name='add', args=arguments(posonlyargs=[], args=[arg(arg='a', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))])), arg(arg='b', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))]))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='out', ctx=Store())], value=BinOp(left=Name(id='a', ctx=Load()), op=Add(), right=Name(id='b', ctx=Load()))), Return(value=Name(id='out', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='cpu'))])])], type_ignores=[])\n",
      "Visit Module\n",
      "Visit FunctionDef\n",
      "Visit arguments\n",
      "Visit Assign\n",
      "Visit Name\n",
      "Visit BinOp\n",
      "Visit Name\n",
      "Visit Name\n",
      "Visit Return\n",
      "Visit Name\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import relax as R\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @R.function\n",
      "    def add(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
      "        out: R.Tensor((2, 3), dtype=\"float32\") = R.add(a, b)\n",
      "        return out\n",
      "After applied passes...\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "# from tvm.script import relax as R\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func(private=True)\n",
      "    def add1(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_add: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
      "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
      "        # with T.block(\"root\"):\n",
      "        for ax0, ax1 in T.grid(T.int64(2), T.int64(3)):\n",
      "            with T.block(\"T_add\"):\n",
      "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
      "                T.reads(a[v_ax0, v_ax1], b[v_ax0, v_ax1])\n",
      "                T.writes(T_add[v_ax0, v_ax1])\n",
      "                T_add[v_ax0, v_ax1] = a[v_ax0, v_ax1] + b[v_ax0, v_ax1]\n",
      "\n",
      "    @R.function\n",
      "    def add(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
      "        cls = Module\n",
      "        out = R.call_tir(cls.add1, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
      "        return out\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import ast\n",
    "import astunparse\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import tvm\n",
    "from tvm import relax as rx\n",
    "from tvm.script import relax as R\n",
    "from tvm.script.ir_builder import relax as relax_builder, ir as I, IRBuilder as IB\n",
    "def jit(target=\"cpu\"):\n",
    "    assert target in [\"cpu\", \"gpu\"]\n",
    "    def inner(fn):\n",
    "        return JIT(fn, target=target)\n",
    "    return inner\n",
    "\n",
    "class JIT:\n",
    "    def __init__(self, fn, target=\"cpu\"):\n",
    "        self.fn = fn\n",
    "        self.target = target\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        fn_src = inspect.getsource(self.fn)\n",
    "        fn_ast = ast.parse(fn_src)\n",
    "        print(ast.dump(fn_ast))\n",
    "        ctx = self.fn.__globals__.copy()\n",
    "        code_generator = CodeGenerator(fn_ast, ctx, self.target)\n",
    "        compiled_kernel = code_generator.code_gen()\n",
    "        input_args = []\n",
    "        for arg in args:\n",
    "            input_args.append(arg.data)\n",
    "        return compiled_kernel(*input_args)\n",
    "\n",
    "class CodeGenerator(ast.NodeVisitor):\n",
    "    def __init__(self, fn_ast, ctx, target):\n",
    "        self.fn_ast = fn_ast\n",
    "        self.ctx = ctx\n",
    "        self.target = target\n",
    "        self.ib = IB()\n",
    "        self.ir_module = None\n",
    "        self.entry = None\n",
    "        self.ret = None\n",
    "        self.local_var_table : Dict[str, Any] = {}\n",
    "    \n",
    "    def code_gen(self):\n",
    "        with self.ib:\n",
    "            self.visit(self.fn_ast)\n",
    "        module = self.ib.get()\n",
    "        print(module)\n",
    "\n",
    "        # apply transform pass on module\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            # Apply Opt Pass\n",
    "            seq = tvm.transform.Sequential(\n",
    "                [\n",
    "                    rx.transform.LegalizeOps(),\n",
    "                ])\n",
    "            module = seq(module)\n",
    "        print(\"After applied passes...\")    \n",
    "        print(module)\n",
    "\n",
    "        mapped_target = {'cpu': 'llvm', 'gpu': 'cuda'}\n",
    "        target = tvm.target.Target(mapped_target[self.target])\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            ex = rx.build(module, target=target)\n",
    "        device = tvm.cuda() if \"cuda\" in target.keys else tvm.cpu()\n",
    "        vm = rx.VirtualMachine(ex, device=device)\n",
    "        return vm[self.entry]\n",
    "\n",
    "\n",
    "    def visit(self, node):\n",
    "        print(\"Visit \" + node.__class__.__name__)\n",
    "        return super().visit(node)\n",
    "    \n",
    "    def visit_Module(self, node: ast.Module):\n",
    "        if self.ir_module:\n",
    "            raise AssertionError(\"We should have only one module!\")\n",
    "        self.ir_module = I.ir_module()\n",
    "        with self.ir_module:\n",
    "            super().generic_visit(node)\n",
    "        \n",
    "    \n",
    "    def visit_FunctionDef(self, node: ast.FunctionDef):\n",
    "        fn = relax_builder.function()\n",
    "        self.entry = node.name\n",
    "        with fn:\n",
    "            R.func_name(node.name)\n",
    "            self.visit(node.args)\n",
    "            self._visit_compound_stmt(node.body)\n",
    "\n",
    "            if self.ret is None:\n",
    "                R.func_ret_value(rx.ShapeExpr([]))\n",
    "            else:\n",
    "                R.func_ret_value(self.ret)\n",
    "    \n",
    "    def visit_arguments(self, node: ast.arguments):\n",
    "        for arg in node.args:\n",
    "            if arg.annotation is None:\n",
    "                raise ValueError(arg, \"Type annotation is required for function parameters.\")\n",
    "            arg_name = arg.arg\n",
    "            # anno = eval(astunparse.unparse(arg.annotation), self.ctx) # self.ctx is used here! \n",
    "            anno = eval(astunparse.unparse(arg.annotation))\n",
    "            # print(self.ctx)\n",
    "            # print(anno)\n",
    "            # 使用eval和astunparse来取得了我们想要的东西, 其实上是得到一个类\n",
    "            param = R.arg(arg_name, R.Tensor(shape=anno.shape, dtype=anno.dtype))\n",
    "            self.local_var_table[arg_name] = param\n",
    "\n",
    "    def visit_Pass(self, node: ast.Pass):\n",
    "        pass\n",
    "\n",
    "    def visit_Assign(self, node: ast.Assign):\n",
    "        if len(node.targets) != 1:\n",
    "            raise NotImplementedError(\"Doesn't support simultaneous multiple assignment like 'a = b = c' in AST node type: {}\".format(type(node).__name__))\n",
    "        target: rx.Var = self.visit(node.targets[0])\n",
    "        value = self.visit(node.value)\n",
    "        self.local_var_table[target.name_hint] = value\n",
    "        self.ib.name(target.name_hint, value)\n",
    "    \n",
    "    def visit_Name(self, node: ast.Name):\n",
    "        name = node.id\n",
    "        if isinstance(node.ctx, ast.Store):\n",
    "            if name not in self.local_var_table.keys():\n",
    "                self.local_var_table[name] = rx.Var(name, struct_info=rx.ObjectStructInfo())\n",
    "        return self.local_var_table[name]\n",
    "    \n",
    "    def visit_BinOp(self, node: ast.BinOp):\n",
    "        lhs = self.visit(node.left)\n",
    "        rhs = self.visit(node.right)\n",
    "        return R.emit(self._binOp_maker(node.op)(lhs, rhs))\n",
    "    \n",
    "    def visit_Return(self, node: ast.Return):\n",
    "        ret_value = self.visit(node.value)\n",
    "        return ret_value\n",
    "    \n",
    "    def visit_Constant(self, node: ast.Constant):\n",
    "        return R.emit(rx.const(node.value))\n",
    "        \n",
    "    def _visit_compound_stmt(self, stmts):\n",
    "        assert isinstance(stmts, (list, tuple))\n",
    "        for stmt in stmts:\n",
    "            ret = self.visit(stmt)\n",
    "            if ret is not None and isinstance(stmt, ast.Return):\n",
    "                self.ret = ret\n",
    "    \n",
    "    def _binOp_maker(self, node: ast.operator):\n",
    "        if isinstance(node, ast.Add):\n",
    "            return R.add\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "    \n",
    "    def generic_visit(self, node: ast.AST):\n",
    "        raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, shape, dtype):\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self._data = None\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    @data.setter\n",
    "    def data(self, data: \"torch.Tensor\"):\n",
    "        def _from_dlpack(tensor):\n",
    "            from tvm.runtime import Device\n",
    "            from tvm.runtime import ndarray\n",
    "            try:\n",
    "                return ndarray.from_dlpack(tensor)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "            device_type = tensor.device.type\n",
    "            device_id = tensor.device.index or 0\n",
    "            return ndarray.array(\n",
    "                tensor.numpy(),\n",
    "                device=Device(\n",
    "                    Device.STR2MASK[device_type],\n",
    "                    device_id,\n",
    "                ),\n",
    "            )\n",
    "        data = _from_dlpack(data)\n",
    "        if data.shape != tuple(self.shape):\n",
    "            raise ValueError(f\"Shape mismatch: expected {tuple(self.shape)}, got {data.shape}\")\n",
    "        if data.dtype != self.dtype:\n",
    "            raise ValueError(f\"Dtype mismatch: expected {self.dtype}, got {data.dtype}\")\n",
    "        self._data = data\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dtype) + '[' + ', '.join(str(s) for s in self.shape) + ']'\n",
    "\n",
    "@jit(target=\"cpu\")\n",
    "def add(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")): # NOTE: TVM 支持 dynamic shape! See DynTensorType\n",
    "    out = a + b\n",
    "    return out\n",
    "\n",
    "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "a.data = torch.ones(size=(2, 3), dtype=torch.float32)\n",
    "b.data = torch.ones(size=(2, 3), dtype=torch.float32)\n",
    "print(add(a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyscf_isdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
