{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import ast\n",
    "import astunparse\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import tvm\n",
    "from tvm import dlight as dl\n",
    "from tvm import relax as rx\n",
    "from tvm.script import relax as R\n",
    "from tvm.script.ir_builder import relax as relax_builder, ir as I, IRBuilder as IB\n",
    "def jit(target=\"cpu\"):\n",
    "    assert target in [\"cpu\", \"gpu\"]\n",
    "    def inner(fn):\n",
    "        return JIT(fn, target=target)\n",
    "    return inner\n",
    "\n",
    "class JIT:\n",
    "    def __init__(self, fn, target=\"cpu\"):\n",
    "        self.fn = fn\n",
    "        self.target = target\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        fn_src = inspect.getsource(self.fn)\n",
    "        fn_ast = ast.parse(fn_src)\n",
    "        print(ast.dump(fn_ast))\n",
    "        ctx = self.fn.__globals__.copy()\n",
    "        code_generator = CodeGenerator(fn_ast, ctx, self.target)\n",
    "        compiled_kernel = code_generator.code_gen()\n",
    "        input_args = []\n",
    "        for arg in args:\n",
    "            input_args.append(arg.data)\n",
    "        return compiled_kernel(*input_args)\n",
    "\n",
    "class CodeGenerator(ast.NodeVisitor):\n",
    "    def __init__(self, fn_ast, ctx, target):\n",
    "        self.fn_ast = fn_ast\n",
    "        self.ctx = ctx\n",
    "        self.target = target\n",
    "        self.ib = IB()\n",
    "        self.ir_module = None\n",
    "        self.entry = None\n",
    "        self.ret = None\n",
    "        self.local_var_table : Dict[str, Any] = {}\n",
    "    \n",
    "    def code_gen(self):\n",
    "        with self.ib:\n",
    "            self.visit(self.fn_ast)\n",
    "        module = self.ib.get()\n",
    "        print(module)\n",
    "\n",
    "        # apply transform pass on module\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            # Apply Opt Pass\n",
    "            seq = tvm.transform.Sequential(\n",
    "                [\n",
    "                    rx.transform.ConvertToDataflow(),\n",
    "                    # 这里其实稍微需要注意的是ConvertToDataflow，它会将我们函数中，没有SideEffect的操作融合在一个Dataflow中（如没有I/O操作，If/Else等Control flow）\n",
    "                    rx.transform.LegalizeOps(),\n",
    "                    rx.transform.AnnotateTIROpPattern(),\n",
    "                    rx.transform.FuseOps(),\n",
    "                    rx.transform.FuseTIR(),\n",
    "                ])\n",
    "            module = seq(module)\n",
    "        print(\"After applied passes...\")    \n",
    "        print(module)\n",
    "\n",
    "        mapped_target = {'cpu': 'llvm', 'gpu': 'cuda'}\n",
    "        target = tvm.target.Target(mapped_target[self.target])\n",
    "        if \"cuda\" in target.keys:\n",
    "            with target:\n",
    "                module = dl.ApplyDefaultSchedule(dl.gpu.Fallback(),)(module)\n",
    "            print(\"After applied dlight...\")\n",
    "            print(module)\n",
    "        \n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            ex = rx.build(module, target=target)\n",
    "\n",
    "        if \"cuda\" in target.keys:\n",
    "            # dump cuda source\n",
    "            print(ex.mod.imported_modules[0].imported_modules[0].get_source())\n",
    "                \n",
    "        device = tvm.cuda() if \"cuda\" in target.keys else tvm.cpu()\n",
    "        vm = rx.VirtualMachine(ex, device=device)\n",
    "        return vm[self.entry]\n",
    "\n",
    "\n",
    "    def visit(self, node):\n",
    "        print(\"Visit \" + node.__class__.__name__)\n",
    "        return super().visit(node)\n",
    "    \n",
    "    def visit_Module(self, node: ast.Module):\n",
    "        if self.ir_module:\n",
    "            raise AssertionError(\"We should have only one module!\")\n",
    "        self.ir_module = I.ir_module()\n",
    "        with self.ir_module:\n",
    "            super().generic_visit(node)\n",
    "        \n",
    "    \n",
    "    def visit_FunctionDef(self, node: ast.FunctionDef):\n",
    "        fn = relax_builder.function()\n",
    "        self.entry = node.name\n",
    "        with fn:\n",
    "            R.func_name(node.name)\n",
    "            self.visit(node.args)\n",
    "            self._visit_compound_stmt(node.body)\n",
    "\n",
    "            if self.ret is None:\n",
    "                R.func_ret_value(rx.ShapeExpr([]))\n",
    "            else:\n",
    "                R.func_ret_value(self.ret)\n",
    "    \n",
    "    def visit_arguments(self, node: ast.arguments):\n",
    "        for arg in node.args:\n",
    "            if arg.annotation is None:\n",
    "                raise ValueError(arg, \"Type annotation is required for function parameters.\")\n",
    "            arg_name = arg.arg\n",
    "            anno = eval(astunparse.unparse(arg.annotation), self.ctx)\n",
    "            param = R.arg(arg_name, R.Tensor(shape=anno.shape, dtype=anno.dtype))\n",
    "            self.local_var_table[arg_name] = param\n",
    "\n",
    "    def visit_Pass(self, node: ast.Pass):\n",
    "        pass\n",
    "\n",
    "    def visit_Assign(self, node: ast.Assign):\n",
    "        if len(node.targets) != 1:\n",
    "            raise NotImplementedError(\"Doesn't support simultaneous multiple assignment like 'a = b = c' in AST node type: {}\".format(type(node).__name__))\n",
    "        target: rx.Var = self.visit(node.targets[0])\n",
    "        value = self.visit(node.value)\n",
    "        self.local_var_table[target.name_hint] = value\n",
    "        self.ib.name(target.name_hint, value)\n",
    "    \n",
    "    def visit_Name(self, node: ast.Name):\n",
    "        name = node.id\n",
    "        if isinstance(node.ctx, ast.Store):\n",
    "            if name not in self.local_var_table.keys():\n",
    "                self.local_var_table[name] = rx.Var(name, struct_info=rx.ObjectStructInfo())\n",
    "        return self.local_var_table[name]\n",
    "    \n",
    "    def visit_BinOp(self, node: ast.BinOp):\n",
    "        lhs = self.visit(node.left)\n",
    "        rhs = self.visit(node.right)\n",
    "        return R.emit(self._binOp_maker(node.op)(lhs, rhs))\n",
    "    \n",
    "    def visit_Return(self, node: ast.Return):\n",
    "        ret_value = self.visit(node.value)\n",
    "        return ret_value\n",
    "    \n",
    "    def visit_Constant(self, node: ast.Constant):\n",
    "        return R.emit(rx.const(node.value))\n",
    "        \n",
    "    def _visit_compound_stmt(self, stmts):\n",
    "        assert isinstance(stmts, (list, tuple))\n",
    "        for stmt in stmts:\n",
    "            ret = self.visit(stmt)\n",
    "            if ret is not None and isinstance(stmt, ast.Return):\n",
    "                self.ret = ret\n",
    "    \n",
    "    def _binOp_maker(self, node: ast.operator):\n",
    "        if isinstance(node, ast.Add):\n",
    "            return R.add\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "    \n",
    "    def generic_visit(self, node: ast.AST):\n",
    "        raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, shape, dtype):\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self._data = None\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    \n",
    "    @data.setter\n",
    "    def data(self, data: \"torch.Tensor\"):\n",
    "        def _from_dlpack(tensor):\n",
    "            from tvm.runtime import Device\n",
    "            from tvm.runtime import ndarray\n",
    "            try:\n",
    "                return ndarray.from_dlpack(tensor)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "            device_type = tensor.device.type\n",
    "            device_id = tensor.device.index or 0\n",
    "            return ndarray.array(\n",
    "                tensor.numpy(),\n",
    "                device=Device(\n",
    "                    Device.STR2MASK[device_type],\n",
    "                    device_id,\n",
    "                ),\n",
    "            )\n",
    "        data = _from_dlpack(data)\n",
    "        if data.shape != tuple(self.shape):\n",
    "            raise ValueError(f\"Shape mismatch: expected {tuple(self.shape)}, got {data.shape}\")\n",
    "        if data.dtype != self.dtype:\n",
    "            raise ValueError(f\"Dtype mismatch: expected {self.dtype}, got {data.dtype}\")\n",
    "        self._data = data\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dtype) + '[' + ', '.join(str(s) for s in self.shape) + ']'\n",
    "\n",
    "@jit(target=\"gpu\")\n",
    "def add(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")):\n",
    "    out = a + b\n",
    "    out = out + a\n",
    "    return out\n",
    "\n",
    "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
    "a.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
    "b.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
    "print(add(a, b))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
